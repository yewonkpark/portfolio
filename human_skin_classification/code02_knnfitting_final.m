%% please DO NOT clear the workspace! the code is based on the training/test set generated by code01_bestmodel

%% (hyper)parameter setting
NSMethod = ["kdtree", "exhaustive"];
Bucketsize = [10,20,30,50,100]; %for kdtree
Distance = {'euclidean','cityblock','chebychev','minkowski'} 
Exponent = [0.5,0.75,1.25,1.5,1,75,3,4]; %for minkowski
NumNeighbors = [1, 3, 5, 7, 9, 11, 13, 15];


%% Training Knn model with 1) KDTree hyperparameter adjustment (Distance Method and Number of Neighbor)

% We train the different KNN Model while varying the parameter of
% the distance method (with exponent adjustment for minkowski method) and number of neighbor

% Creating a cell for storing the performance of each model 

DM = size(Distance,2) +size(Exponent,2) -2 %rows for the cell (number of Method parameters, -2 for varying exponent for minkowski)
NN = size(NumNeighbors,2) %columns for the cell (number of neighbor parameters)
CModel = cell(DM,NN) %model 
CTime = cell(DM,NN) %computation time
CMisclassification = cell(DM,NN) %number of misclassification
CErrrate = cell(DM,NN) %error rate
            

for i = 1:DM;
    for k = 1:size(NumNeighbors,2);
        % Generate Model with different parameters setting
        if i >= 4;
            CModel{i,k} = fitcknn(dataTrain, 'Skinclass','NSMethod','kdtree','Distance',Distance(4),'NumNeighbors',NumNeighbors(k),'Exponent',Exponent(i-3));
        else;
            CModel{i,k} = fitcknn(dataTrain, 'Skinclass','NSMethod','kdtree','Distance',Distance(i),'NumNeighbors',NumNeighbors(k));
        end
        
        % Prediction for test data and measurement of computation time
        tic;
        prediction = predict(CModel{i,k}, dataTest);
        CTime{i,k} = toc  
        
        % Evaluation : error-rate for K-Nearest Neighbor
        CErrrate{i,k} = loss(CModel{i,k}, dataTest)
        
        % Generating confusion matrix 
        [result,class] = confusionmat(dataTest.Skinclass, prediction) %prediction result
        
        % Number of misclassification
        CMisclassification{i,k} = result(1,2) + result(2,1)
    end
end


%% Training Knn model 2) Exhaustive vs KDTree comparison

% We train and compare kNN Models using Exhaustive vs KDTree as neighbor
% search method also with different number of neighbor setting
% HOWEVER, since Two search method does not yield any different performance for our dataset, 
% the code is converted into COMMENT
% The code can be run by deleting % comment in every line

% Creating a cell for storing the performance of each model

%EK_DM = size(NSMethod,2) %rows for the cell (two NSMethod)
%EK_NN = size(NumNeighbors,2) %columns for the cell (number of neighbor parameters)
%EK_CModel = cell(EK_DM,EK_NN)%model 
%EK_CTime = cell(EK_DM,EK_NN)%computation time
%EK_CMisclassification = cell(EK_DM,EK_NN) %number of misclassification
%EK_CErrrate = cell(EK_DM,EK_NN) %error rate


%for i = 1:size(NSMethod,2);
    %for k = 1:size(NumNeighbors,2);
     % Generate Model with different parameters setting
       % EK_CModel{i,k} = fitcknn(dataTrain, 'Skinclass','NSMethod',NSMethod(i),'Distance','euclidean','NumNeighbors',NumNeighbors(k));
       
       % Prediction for test data and measurement of computation time
       % tic;
       % prediction = predict(EK_CModel{i,k}, dataTest); 
       % EK_CTime{i,k} = toc
       
       
       % Generating confusion matrix
       % [result,class] = confusionmat(dataTest.Skinclass, prediction) 
       
       % Evaluation : error-rate for K-Nearest Neighbor
       % EK_CErrrate{i,k} = loss(EK_CModel{i,k}, dataTest)
       
       % Number of misclassification
       % EK_CMisclassification{i,k} = result(1,2) + result(2,1)

    %end
%end

%% Training Knn model 3) KDTree Bucket size parameter

% We train and compare kNN Models with different KDTree Bucket size parameters
% HOWEVER, since these Bucket Size adjustment does not result in any difference for our dataset,
% this code is converted into COMMENT
% The code can be run by deleting % comment in every line

% Creating a cell for storing the performance of each model

%K_BS = size(Bucketsize,2)
%K_BSModel = cell(1,K_BS)
%K_BSTime = cell(1,K_BS)
%K_BSMisclassification = cell(1,K_BS)
%K_BSErrrate = cell(1,K_BS)



%for k = 1:size(Bucketsize,2);
 %   Generate Model with different parameters setting
 %   K_BSModel{k} = fitcknn(dataTrain, 'Skinclass','NSMethod','kdtree','Distance','euclidean','NumNeighbors',3,'BucketSize',Bucketsize(k));
 
 %   Prediction for test data and measurement of computation time
 %   tic;
 %   prediction = predict(K_BSModel{k}, dataTest); 
 %   K_BSTime{k} = toc ;
 
 %   Generating confusion matrix
 %   [result,class] = confusionmat(dataTest.Skinclass, prediction)
 
 %   Evaluation : error-rate for K-Nearest Neighbor
 %   K_BSErrrate{k} = loss(K_BSModel{k}, dataTest)
 
 %   Number of misclassification
 %   K_BSMisclassification{k} = result(1,2) + result(2,1)
 
%end




