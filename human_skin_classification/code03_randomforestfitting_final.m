%% please DO NOT clear the workspace! the code is based on the training/test set generated by code01_bestmodel

%% (hyper)parameter setting for Random Forest
 
MaxNumSplits = [10,20,30,40,50]; 
MinLeafSize = [10,50,100, 200, 500];
EnAggMethod = {'AdaBoostM1','LogitBoost','GentleBoost','RUSBoost','Bag'}
NumLearningCycles = [5, 10, 20, 30, 40, 50, 100, 500];

%% Train random forest 1) with different MaxNumSplit & MinLeafSize parameter
% We train the different Random Forest Model while varying the parameter of
% the tree (MaxNumSplit & MinLeafSize)

% Creating a cell for storing the performance of each model

RF = size(MaxNumSplits,2) %rows for the cell (number of MaxNumSplits parameters)
ML = size(MinLeafSize,2) %columns for the cell (number of MinLeafSize parameters)
RFModel = cell(RF,ML) %model
RFTime = cell(RF,ML) %computation time
RFMisclassification = cell(RF,ML) %number of misclassification
RFErrrate = cell(RF,ML) %error rate


for i = 1:RF;
    for k = 1:ML;
        
        % Generate Model with different parameters setting
        Tree = templateTree('MaxNumSplits',MaxNumSplits(i),'MinParentSize',2,'MinLeafSize',MinLeafSize(k)); %set MinParentSize for 2
        RFModel{i,k} = fitcensemble(dataTrain, 'Skinclass','Method','GentleBoost','NumLearningCycles',50,'Learners',Tree);           
        
        % Prediction for test data and measurement of computation time
        tic;
        prediction = predict(RFModel{i,k}, dataTest); 
        RFTime{i,k} = toc
        
        % Evaluation : error-rate for Random Forest
        RFErrrate{i,k} = loss(RFModel{i,k}, dataTest)
        
        % Generating confusion matrix 
        [result,class] = confusionmat(dataTest.Skinclass, prediction)
        
        % Number of misclassification
        RFMisclassification{i,k} = result(1,2) + result(2,1)
        
    end
end



%% Train random forest 2) with different Method & NumLearningCycles parameter 
% We train the different Random Forest Model while varying the different RF
% Method and Number of Learning Cycles
% Here, we use with MaxNumSPlit 20, MinLeafSize200 which generates the best
% model in the experiment avobe


% Creating a cell for storing the performance of each model

MT = size(EnAggMethod,2) %rows for the cell (number of Methods parameters)
LC = size(NumLearningCycles,2) %columns for the cell (number of LearningCycle parameters)
RF2Model = cell(MT,LC) %model
RF2Time = cell(MT,LC) %computation time
RF2Misclassification = cell(MT,LC) %number of misclassification
RF2Errrate = cell(MT,LC)%error rate


for i = 1:MT;
    for k = 1:LC;
        % Generate Model with different parameters setting
        Tree = templateTree('MaxNumSplits',10,'MinParentSize',2,'MinLeafSize',200) %set MinParentSize for 2
        RF2Model{i,k} = fitcensemble(dataTrain, 'Skinclass','Method',EnAggMethod{i},'NumLearningCycles',NumLearningCycles(k),'Learners',Tree)           
        
        % Prediction for test data and measurement of computation time
        tic;
        prediction = predict(RF2Model{i,k}, dataTest);
        RF2Time{i,k} = toc
        
        % Evaluation : error-rate for Random Forest
        RF2Errrate{i,k} = loss(RF2Model{i,k}, dataTest)
        
        % Generating confusion matrix 
        [result,class] = confusionmat(dataTest.Skinclass, prediction)
        
        % Number of misclassification
        RF2Misclassification{i,k} = result(1,2) + result(2,1)
        
    end
end

